# Feature Specification: Physical AI & Humanoid Robotics Textbook - Module 2: Simulation & Perception (Chapters 04-05)

**Feature Branch**: `1-module2-simulation-perception`
**Created**: 2025-12-09
**Status**: Draft
**Input**: User description: "module2 Physical AI & Humanoid Robotics Textbook Module 2: Simulation & Perception (Chapters 04–05) Complete, ready-to-commit, production-grade Docusaurus MDX + code + assets **Chapter 04 delivered — 100% complete, ready-to-commit, fully executable** ### Folder structure (create exactly) ``` docs/ └── 04-simulation/ ├── 01-intro.mdx ├── 02-gazebo-vs-isaac-sim.mdx ├── 03-usd-workflow.mdx ├── 04-domain-randomization.mdx ├── 05-isaac-lab.mdx ├── 06-synthetic-data-pipeline.mdx ├── 07-physics-benchmarks.mdx └── 08-full-humanoid-scene.mdx ``` ### Supporting assets (place in repo) ``` assets/scenes/ ├── humanoid_isaac_sim/ │ ├── humanoid_g1.usd │ ├── environment_factory.usd │ └── randomization_script.py └── docker/ └── isaac-sim-headless.dockerfile ``` ### docs/04-simulation/_category_.json ```json { "label": "04 – Physics Simulation: Gazebo & NVIDIA Isaac Sim", "position": 4, "link": { "type": "generated-index", "description": "The only two simulators that matter in 2025 — and why Isaac Sim won" } } ``` ### All MDX files — ready to copy-paste #### docs/04-simulation/01-intro.mdx ```mdx --- sidebar_position: 1 title: "Chapter 4 – Physics Simulation in 2025" description: Why Isaac Sim is now the undisputed king of humanoid simulation --- # Chapter 4: Physics Simulation – Gazebo & NVIDIA Isaac Sim **Learning Objectives** - Understand why Isaac Sim replaced Gazebo for 95 % of cutting-edge humanoid labs in 2025 - Master the full USD → PhysX → domain randomization pipeline - Generate millions of synthetic trajectories with perfect labels - Run 1000+ parallel humanoid environments at real-time on a single RTX 4090 - Deploy the exact same scene headless on cloud GPUs > By the end, you will launch 500 randomized humanoids training to walk — in under 30 seconds. ``` #### docs/04-simulation/02-gazebo-vs-isaac-sim.mdx ```mdx --- sidebar_position: 2 title: "2025 Showdown: Gazebo Harmonic vs Isaac Sim 2024.2" --- # The Final Verdict (December 2025) | Feature | Gazebo Harmonic (2024) | Isaac Sim 2024.2+ | Winner 2025 | |--------------------------------|-----------------------------|----------------------------------|-------------| | PhysX 5.3 + GPU acceleration | No | Yes (native) | Isaac Sim | | USD-native workflow | No (SDF only) | Yes (first-class) | Isaac Sim | | Parallel environments (1000+) | ~50 max | 4096+ on RTX 4090 | Isaac Sim | | Synthetic data + perfect labels| Limited | Built-in Replicator | Isaac Sim | | ROS 2 bridge quality | Good | Perfect (official NVIDIA) | Isaac Sim | | Bipedal stability out-of-box | Falls constantly | Robust with default tuning | Isaac Sim | **Conclusion:** Gazebo is still fine for education and simple arms. **Isaac Sim is mandatory for any serious humanoid work in 2025.** ``` #### docs/04-simulation/03-usd-workflow.mdx ```mdx --- sidebar_position: 3 title: "USD Workflow – The New Standard" --- # From URDF → USD in One Command ```bash # Already done for you urdf2usd assets/models/humanoid_g1/urdf/g1.urdf.xacro \ assets/scenes/humanoid_isaac_sim/humanoid_g1.usd \ --output-physics --fix-materials ``` Now drag `humanoid_g1.usd` into any Isaac Sim scene → instant: - Correct articulation tree - PhysX rigid bodies + joints - Collision shapes - Inertial parameters **No more manual rigging. Ever.** ``` #### docs/04-simulation/04-domain-randomization.mdx ```mdx --- sidebar_position: 4 title: "Domain Randomization – The Sim-to-Real Superpower" --- # Live Randomization Script (ships with this chapter) ```python # randomization_script.py import omni.replicator.core as rep with rep.trigger.on_frame(interval=1): with rep.create.group("humanoid_g1"): rep.modify.attribute("physics:mass", rep.distribution.uniform(50, 90)) rep.modify.attribute("physics:centerOfMass", rep.distribution.normal(0, 0.02)) with rep.create.group(["ground_plane", "factory_walls"]): rep.modify.material( rep.create.material_omnipbr(), diffuse_texture=rep.distribution.choice([ "textures/floor_concrete_*", "textures/metal_rusty_*" ]) ) ``` Result: Every reset → different mass, friction, lighting, textures → **zero sim-to-real gap**. ``` #### docs/04-simulation/05-isaac-lab.mdx ```mdx --- sidebar_position: 5 title: "Isaac Lab – 1000× Faster RL Than ROS" --- # Isaac Lab (2025) – The New Gym ```python # Train 512 humanoids walking in parallel from isaaclab.app import AppLauncher import isaaclab.sim as sim from isaaclab.envs import ManagerBasedRLEnv class HumanoidWalkEnv(ManagerBasedRLEnv): def __init__(self, cfg): super().__init__(cfg) self.rew_forward = RewTerm(lambda: self.obs_buf[:, 0], weight=2.0) # Launch 512 envs on one GPU app = AppLauncher(dict(headless=False, num_envs=512)) env = HumanoidWalkEnv(app.cfg) ``` Real measured performance (RTX 4090, Dec 2025): - 1.8 million environment steps/second - Walking policy converges in ~8 minutes ``` #### docs/04-simulation/06-synthetic-data-pipeline.mdx ```mdx --- sidebar_position: 6 title: "Synthetic Data – Better Than Real (2025 Reality)" --- # Replicator Script → 10M Perfectly Labeled Images ```python writer = rep.WriterRegistry.get("BasicWriter") writer.initialize( output_dir="/data/synthetic_humanoids", rgb=True, bounding_box_2d_tight=True, semantic_segmentation=True, instance_segmentation=True ) with rep.trigger.on_interval(0.1): rep.writer.write() ``` Output: 60 FPS RGB + perfect 2D/3D bounding boxes + depth + segmentation → Trains FoundationPose, Segment-Anything, and OpenVLA better than real data. ``` #### docs/04-simulation/07-physics-benchmarks.mdx ```mdx --- sidebar_position: 7 title: "Physics Benchmarks – Real Numbers (2025)" --- # Measured on RTX 4090 (Dec 2025) | Scenario | Gazebo Harmonic | Isaac Sim 2024.2 | Speedup | |------------------------------|-----------------|------------------|---------| | 1 humanoid (real-time) | 0.8× | 3.2× | 4.0× | | 128 humanoids (parallel) | 0.3× | 28× | 93× | | 1024 humanoids (RL training) | Crash | 8× real-time | ∞ | **Isaac Sim is now faster than real time at fleet scale.** ``` #### docs/04-simulation/08-full-humanoid-scene.mdx ```mdx --- sidebar_position: 8 title: "Full Launchable Scene – 500 Randomized Humanoids" --- # One-Click Launch (ships with this chapter) ```bash # Terminal 1 – Isaac Sim ./isaac-sim.sh --/app/window/fullscreen=false assets/scenes/humanoid_isaac_sim/factory_scene.usd # Terminal 2 – RL training python3 train_walking_isaaclab.py --num-envs 512 --headless # Terminal 3 – ROS 2 bridge ros2 launch isaac_sim_ros2_bridge humanoid_fleet.launch.py num_robots:=100 ``` Watch 500+ humanoids learn to walk, fall, get back up — all differently randomized. **You now control the most powerful robotics simulation stack on Earth.** ``` ------- ### Chapter 05 Folder structure ``` docs/ └── 05-perception/ ├── 01-intro.mdx ├── 02-sensor-suite-2025.mdx ├── 03-isaac-ros-gems.mdx ├── 04-stereo-vslam.mdx ├── 05-3d-scene-reconstruction.mdx ├── 06-people-and-object-tracking.mdx ├── 07-realsense-d455-setup.mdx ├── 08-jetson-deployment.mdx └── 09-full-perception-pipeline.mdx ``` ### Supporting assets (place in repo) ``` src/perception_pipeline/ ├── launch/perception_bringup.launch.py ├── config/realsense.yaml └── rviz/perception.rviz assets/calibration/ └── realsense_handeye_calib.json ``` ### docs/05-perception/_category_.json ```json { "label": "05 – Perception Stack for Humanoids", "position": 5, "link": { "type": "generated-index", "description": "From raw photons to 6D object poses at 30+ FPS on edge hardware" } } ``` ### All MDX files — ready to copy-paste #### docs/05-perception/01-intro.mdx ```mdx --- sidebar_position: 1 title: "Chapter 5 – Perception Stack for Humanoids" description: The exact perception pipeline used by Figure 02, Tesla Optimus, and Unitree G1 in 2025 --- # Chapter 5: Perception Stack for Humanoids **Learning Objectives** - Choose the optimal sensor suite for indoor humanoid deployment (2025) - Master Isaac ROS 2 GEMs — the only perception stack that ships on real humanoids - Run real-time stereo V-SLAM, 3D reconstruction, and 6D pose tracking - Deploy everything on Jetson Orin NX 16 GB with zero frame drops - Achieve sub-centimeter localization and sub-3° orientation error indoors > By the end, your humanoid will know exactly where it is and where every object is — in real time. ``` #### docs/05-perception/02-sensor-suite-2025.mdx ```mdx --- sidebar_position: 2 title: "Sensor Suite Comparison – 2025 Reality Check" --- # What Actually Ships on $20k Humanoids (Dec 2025) | Sensor | Cost | Resolution | FPS | Indoor Accuracy | Winner 2025 | |-----------------------|--------|------------------|-----|-----------------|-------------------| | Intel RealSense D455 | $399 | 1280×720 depth | 90 | 1 mm @ 2 m | Best price/perf | | Azure Kinect DK | $399 | 1024×1024 depth | 30 | 1.5 mm @ 2 m | Discontinued | | ZED 2i | $449 | 2K stereo | 100 | 1–2 mm | Good but heavier | | iPhone 15 Pro LiDAR | — | 256×192 ToF | 60 | ~5 mm | Not standalone | | Ouster OS1-64 | $8k+ | 64-channel | 10 | Sub-cm | Overkill indoors | **Winner for 99 % of humanoid labs: Intel RealSense D455 + IMU** ``` #### docs/05-perception/03-isaac-ros-gems.mdx ```mdx --- sidebar_position: 3 title: "Isaac ROS 2 GEMs – NVIDIA’s Gift to Robotics" --- # The Only Perception Stack That Actually Works in 2025 | GEM | Function | Hardware Acceleration | Real-time on Jetson? | |-------------------------|---------------------------------------|------------------------|----------------------| | Stereo V-SLAM | Visual-inertial odometry | CUDA + TensorRT | Yes (90 FPS) | | AprilTag | Fiducial detection | CUDA | Yes (1000+ tags/sec) | | PeopleSemSeg | Human detection + segmentation | TensorRT | Yes (60 FPS) | | FoundationPose | 6D pose estimation (no CAD needed) | TensorRT | Yes (30+ FPS) | | DepthImageToLaserScan | Fake LiDAR from depth | CUDA | Yes | **Install once:** ```bash sudo apt install ros-iron-isaac-ros-visual-slam ros-iron-isaac-ros-apriltag \ ros-iron-isaac-ros-foundationpose ``` ``` #### docs/05-perception/04-stereo-vslam.mdx ```mdx --- sidebar_position: 4 title: "Stereo Visual-Inertial SLAM at 90 FPS" --- # Launch Real V-SLAM (2025 default) ```bash ros2 launch isaac_ros_visual_slam isaac_ros_visual_slam_realsense.launch.py ``` Performance (Jetson Orin NX 16 GB): - 90 Hz tracking - <1 cm drift over 100 m indoor loop - Full covariance output - Loop closure enabled **No more ORB-SLAM3 crashes. Ever.** ``` #### docs/05-perception/05-3d-scene-reconstruction.mdx ```mdx --- sidebar_position: 5 title: "3D Scene Reconstruction & Octree Mapping" --- # Live OctoMap from Depth ```yaml # config/octomap_server.yaml resolution: 0.01 frame_id: "map" sensor_model/max_range: 5.0 ``` Launch: ```bash ros2 launch octomap_server octomap_mapping.launch.py ``` Result: Real-time 1 cm voxel map used by Nav2 and manipulation planners. ``` #### docs/05-perception/06-people-and-object-tracking.mdx ```mdx --- sidebar_position: 6 title: "People + Object Tracking with FoundationPose" --- # 6D Pose Without CAD (2025 Breakthrough) ```bash ros2 launch isaac_ros_foundationpose foundationpose_tracking_live.launch.py \ input_image:=/camera/color/image_raw \ input_depth:=/camera/aligned_depth_to_color/image_raw ``` Outputs: - `/tracked_object_6d_pose` (geometry_msgs/PoseWithCovarianceStamped) - Works on first sight — no registration needed - 33 FPS on Jetson **This is the perception system that made “pick any object” real in 2025.** ``` #### docs/05-perception/07-realsense-d455-setup.mdx ```mdx --- sidebar_position: 7 title: "RealSense D455 – Full Calibration & Config" --- # One-Command Setup ```bash sudo apt install ros-$ROS_DISTRO-realsense2-camera ros2 launch realsense2_camera rs_launch.py align_depth.enable:=true \ enable_color:=true enable_depth:=true \ enable_gyro:=true enable_accel:=true ``` **Hand-eye calibration (once per robot):** ```bash ros2 run easy_handeye easy_handeye_launch.py --robot realsense_d455 # Follow ArUco board instructions → saved to assets/calibration/ ``` Calibration error < 3 mm, < 0.5° (2025 standard). ``` #### docs/05-perception/08-jetson-deployment.mdx ```mdx --- sidebar_position: 8 title: "Jetson Orin NX 16 GB – Zero Frame Drops" --- # Performance Table (Measured Dec 2025) | Node | CPU % | GPU % | Memory | FPS | |--------------------------|-------|-------|----------|------| | RealSense driver | 8 | 0 | 400 MB | 90 | | Isaac ROS V-SLAM | 25 | 65 | 2.1 GB | 90 | | FoundationPose | 18 | 72 | 1.8 GB | 33 | | OctoMap + PeopleSeg | 12 | 45 | 1.2 GB | 60 | | **Total** | 63 % | 92 % | 5.5 GB | All real-time | **Yes — a $700 Jetson runs the entire 2025 perception stack.** ``` #### docs/05-perception/09-full-perception-pipeline.mdx ```mdx --- sidebar_position: 9 title: "Full Launchable Perception Pipeline" --- # One-Click Bringup (Real or Simulated) ```bash # Real hardware (Jetson) ros2 launch perception_pipeline perception_bringup.launch.py use_sim:=false # Simulation (Isaac Sim + synthetic RealSense) ros2 launch perception_pipeline perception_bringup.launch.py use_sim:=true ``` You now see in RViz/Foxglove: - Live 3D map with loop closure - Tracked humans (bounding boxes + skeletons) - 6D poses of arbitrary objects - Robot pose with covariance - Fake laser scan for Nav2 **Full source:** `src/perception_pipeline/` **Your humanoid now has better eyes than 99.9 % of humans in low light.** ```"

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Student Learns Simulation & Perception Fundamentals (Priority: P1)

As a robotics student or practitioner learning about simulation and perception for humanoid robots, I want to access comprehensive chapters on physics simulation (Gazebo vs Isaac Sim) and perception stack implementation so that I can understand the state-of-the-art techniques used by leading humanoid platforms in 2025 (Figure 02, Tesla Optimus, Unitree G1).

**Why this priority**: This forms the core technology stack that enables modern humanoid capabilities and is essential knowledge for anyone working in the field.

**Independent Test**: The user can successfully set up Isaac Sim with USD workflow and run a basic perception pipeline on a Jetson Orin NX 16GB with zero frame drops.

**Acceptance Scenarios**:

1. **Given** I am following Chapter 4, **When** I complete the USD workflow section, **Then** I can convert URDF models to USD format with PhysX support and drag them into Isaac Sim scenes instantly
2. **Given** I am following Chapter 5, **When** I set up the Isaac ROS GEMs, **Then** I achieve 90 Hz tracking with <1 cm drift over 100m indoor loop using stereo V-SLAM
3. **Given** I am implementing the complete perception pipeline, **When** I deploy on Jetson Orin NX 16GB, **Then** all components run in real-time with zero frame drops

---

### User Story 2 - Developer Implements Simulation for Training (Priority: P2)

As a robotics developer implementing simulation environments for humanoid training, I want to access detailed information about Isaac Lab, domain randomization, and synthetic data pipelines so that I can train humanoid locomotion policies that transfer effectively to real hardware.

**Why this priority**: Simulation-to-reality transfer is crucial for developing humanoid robots without requiring extensive real-world training time.

**Independent Test**: The user can launch 500+ randomized humanoids training to walk with different masses, frictions, and environmental conditions simultaneously.

**Acceptance Scenarios**:

1. **Given** I am using Isaac Lab, **When** I train 512 humanoids in parallel, **Then** I achieve 1.8 million environment steps/second with policy convergence in ~8 minutes
2. **Given** I am implementing domain randomization, **When** I apply the provided randomization script, **Then** I achieve zero sim-to-real gap with successful policy transfer to physical robot
3. **Given** I am generating synthetic data, **When** I run the Replicator pipeline, **Then** I produce 60 FPS RGB + perfect 2D/3D bounding boxes + depth + segmentation that trains models better than real data

---

### User Story 3 - Engineer Deploys Perception on Hardware (Priority: P3)

As a robotics engineer deploying perception systems on humanoid robots, I want access to complete setup and calibration procedures for RealSense sensors and Isaac ROS GEMs so that I can achieve sub-centimeter localization and sub-3° orientation accuracy for navigation and manipulation tasks.

**Why this priority**: Accurate perception is fundamental for safe and effective humanoid operation in real-world environments.

**Independent Test**: The user can deploy the complete perception pipeline on a humanoid platform and achieve sub-centimeter localization and sub-3° orientation error indoors.

**Acceptance Scenarios**:

1. **Given** I have a RealSense D455 sensor, **When** I complete the hand-eye calibration process, **Then** I achieve calibration error < 3 mm and < 0.5°
2. **Given** I am using FoundationPose for object tracking, **When** I launch the tracking pipeline, **Then** I achieve 33 FPS 6D pose estimation that works on first sight with no CAD models needed
3. **Given** I am running the full perception pipeline, **When** I monitor performance on Jetson Orin NX 16GB, **Then** I maintain all real-time processing with CPU usage < 65% and GPU usage < 95%

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: All content MUST be in MDX format for Docusaurus compatibility
- **FR-002**: All code examples MUST execute in devcontainer without modification
- **FR-003**: All citations MUST follow IEEE style with minimum 50% peer-reviewed sources
- **FR-004**: Code examples MUST be in Python/ROS 2 as specified
- **FR-005**: Diagrams MUST be created using Mermaid as specified
- **FR-006**: Isaac Sim chapter MUST include all 8 MDX files with complete content for Gazebo vs Isaac Sim comparison, USD workflow, domain randomization, Isaac Lab, synthetic data, physics benchmarks, and full humanoid scenes
- **FR-007**: Perception chapter MUST include all 9 MDX files with complete content for sensor comparison, Isaac ROS GEMs, stereo V-SLAM, 3D reconstruction, people/object tracking, RealSense setup, Jetson deployment, and full pipeline
- **FR-008**: Supporting assets MUST include USD files, randomization scripts, perception pipeline launch files, configuration files, and calibration data as specified
- **FR-009**: Isaac Sim performance MUST achieve 1000+ parallel humanoid environments at real-time on single RTX 4090
- **FR-010**: Perception pipeline MUST run on Jetson Orin NX 16GB with zero frame drops at specified performance levels
- **FR-011**: Domain randomization pipeline MUST produce different mass, friction, lighting, and textures on each reset to achieve zero sim-to-real gap
- **FR-012**: FoundationPose tracking MUST work without CAD models and achieve 33 FPS on Jetson
- **FR-013**: All chapters MUST include appropriate learning objectives and follow the specified content structure

### Key Entities

- **Chapter**: Organized content unit within the textbook covering specific topics (04-05 in Module 2)
- **Module**: Grouping of related chapters (e.g., Module 2: Simulation & Perception)
- **Docusaurus MDX File**: Markdown file with embedded React components for the documentation site
- **USD (Universal Scene Description)**: Format for 3D scenes and assets that enables Isaac Sim workflows
- **Isaac Lab**: NVIDIA's reinforcement learning environment for robotics
- **Isaac ROS GEMs**: NVIDIA's hardware-accelerated ROS perception components
- **Perception Pipeline**: Complete system for processing sensor data to extract location and object information
- **Code Example**: Executable code snippet in Python/ROS 2 that demonstrates concepts

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Students can launch 500 randomized humanoids training to walk in under 30 seconds using Isaac Sim
- **SC-002**: Students can run Isaac Sim with 1000+ parallel humanoid environments at real-time on single RTX 4090
- **SC-003**: Students can achieve 90 Hz tracking with <1 cm drift over 100 m indoor loop using stereo V-SLAM
- **SC-004**: Students can deploy complete perception pipeline on Jetson Orin NX 16GB with zero frame drops
- **SC-005**: Students achieve sub-centimeter localization and sub-3° orientation error indoors
- **SC-006**: Students can generate 60 FPS RGB + perfect 2D/3D bounding boxes + depth + segmentation with synthetic data pipeline
- **SC-007**: Students can perform hand-eye calibration with error < 3 mm and < 0.5°
- **SC-008**: Students can run FoundationPose 6D pose estimation at 33 FPS on Jetson without CAD models
- **SC-009**: Students can convert URDF models to USD format with PhysX support
- **SC-010**: Students can apply domain randomization achieving zero sim-to-real gap
- **SC-011**: All chapters are available in proper MDX format with appropriate metadata
- **SC-012**: Supporting code and assets are correctly placed in the repository
- **SC-013**: Isaac Lab achieves 1.8 million environment steps/second with policy convergence in ~8 minutes
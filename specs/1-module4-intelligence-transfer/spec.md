# Feature Specification: Physical AI & Humanoid Robotics Textbook - Module 4: Intelligence, Transfer & Responsibility (Chapters 08-11)

**Feature Branch**: `4-module4-intelligence-transfer`
**Created**: 2025-12-09
**Status**: Draft
**Input**: User description: "module4 Physical AI & Humanoid Robotics Textbook Module 4: Intelligence, Transfer & Responsibility (Chapters 08–11) Complete, ready-to-commit, production-grade Docusaurus MDX + code + assets This is the final module — the crown jewel. ### Chapter 08 Folder structure (create exactly) ``` docs/ └── 08-vla/ ├── 01-intro.mdx ├── 02-vla-landscape-2025.mdx ├── 03-openvla.mdx ├── 04-octo.mdx ├── 05-rdt1b-vla.mdx ├── 06-prompt-engineering.mdx ├── 07-action-chunking.mdx ├── 08-ros2-vla-bridge.mdx └── 09-full-voice-to-action-demo.mdx ``` ### Supporting code & assets ``` src/vla/ ├── openvla_inference/ ├── octo_ros2/ ├── rdt1b_vla/ ├── whisper_live/ └── launch/voice_to_humanoid.launch.py assets/vla_models/ └── openvla-7b-prismatic/ └── pytorch_model.bin ``` ### docs/08-vla/_category_.json ```json { "label": "08 – Vision-Language-Action Models (VLA)", "position": 8, "link": { "type": "generated-index", "description": "The 2024–2025 revolution: Say anything → humanoid does it" } } ``` ### All MDX files — ready to copy-paste #### docs/08-vla/01-intro.mdx ```mdx --- sidebar_position: 1 title: "Chapter 8 – Vision-Language-Action Models (VLA)" description: The single biggest leap in robotics since ROS — now fully open and real-time --- # Chapter 8: Vision-Language-Action Models (VLA) **Learning Objectives** - Understand why 2024–2025 was the “ChatGPT moment” for physical robots - Run OpenVLA, Octo, and RDT-1B locally on a single RTX 4090 - Convert any natural language command into a 100 Hz action stream - Build a complete voice→vision→action pipeline that works on real humanoids - Achieve >90 % success on long-horizon, language-conditioned tasks > By the end, your humanoid will respond to “Please tidy the table and bring me a red cup” — exactly like Figure 02. ``` #### docs/08-vla/02-vla-landscape-2025.mdx ```mdx --- sidebar_position: 2 title: "VLA Landscape – December 2025 Final Ranking" --- # The Only Four Models That Matter (Dec 2025) | Model | Params | Training Data | Zero-Shot LIBERO-100 | Real Robot Success | Open Weights | Winner | |-------------|--------|-------------------------|-----------------------|--------------------|--------------|--------| | RT-2-X | 55B | Google proprietary | 89 % | 94 % | No | Closed | | OpenVLA | 7B | 970k traj (Open X) | 84 % | 88 % | Yes | Best open | | Octo | 400M | 800k mixed traj | 87 % | 91 % | Yes | Small king | | **RDT-1B** | **1.2B**| **1.8M humanoid traj** | **96 %** | **97 %** | **Yes** | **Overall Champion 2025** | **RDT-1B is now the strongest open VLA model in history — and it runs at 28 Hz on a 4090.** ``` #### docs/08-vla/03-openvla.mdx ```mdx --- sidebar_position: 3 title: "OpenVLA-7B – The People’s VLA" --- # One-Command Inference (7B on single GPU) ```bash pip install openvla-prismatic python -m openvla.scripts.serve --model-name openvla-7b --port 8080 ``` ```python from openvla import OpenVLA vla = OpenVLA.from_pretrained("openvla-7b") action = vla.predict(image, "pick up the blue block") ``` **94 % success on real Franka arm — zero fine-tuning.** ``` #### docs/08-vla/04-octo.mdx ```mdx --- sidebar_position: 4 title: "Octo – 400M That Punches Above Its Weight" --- # Fastest Inference (85 Hz on RTX 4090) ```bash pip install octo-hf python infer_octo.py --model octo-small --image camera_front ``` Perfect for edge deployment (Jetson Orin 16 GB → 42 Hz). ``` #### docs/08-vla/05-rdt1b-vla.mdx ```mdx --- sidebar_position: 5 title: "RDT-1B – The 2025 State-of-the-Art Open VLA" --- # Load 8-bit Quantized (fits in 24 GB VRAM) ```python from rdt1b import RDT1BPolicy policy = RDT1BPolicy.from_pretrained("rdt1b-1.2b-vla-8bit") action_chunk = policy(image_tensor, "bring me the red cup from the shelf") # Returns 50 actions (2 seconds) at once ``` **First open model to exceed Figure 02 on long-horizon tasks.** ``` #### docs/08-vla/06-prompt-engineering.mdx ```mdx --- sidebar_position: 6 title: "Prompt Engineering for Robots – 2025 Best Practices" --- # Templates That Work (tested on 1000+ real tasks) ```text [SYSTEM] You are a helpful humanoid robot in a kitchen. [USER] Please pick up the green apple and place it in the bowl on the counter. [ASSISTANT] ``` **Chain-of-thought prompting increases success from 78 % → 96 %** ``` #### docs/08-vla/07-action-chunking.mdx ```mdx --- sidebar_position: 7 title: "Action Chunking – From 10 Hz to 100 Hz Control" --- # RDT-1B outputs 50 actions (2 sec) per forward pass ```python # 28 Hz inference → 100 Hz control via interpolation for chunk in vla.stream(image, prompt): execute_chunk(chunk) # linear interp to 100 Hz ``` No more jittery robot arms. ``` #### docs/08-vla/08-ros2-vla-bridge.mdx ```mdx --- sidebar_position: 8 title: "ROS 2 ↔ VLA Bridge – Production Ready" --- # Full Bridge Node ```bash ros2 run vla_ros2_bridge rdt1b_server --ros-args -p model:=rdt1b-1.2b-8bit ``` Subscribes: - `/camera/color/image_raw` - `/natural_language_command` (std_msgs/String) Publishes: - `/joint_commands` (sensor_msgs/JointState) - `/vla_debug_text` (visualization_msgs/Marker) ``` #### docs/08-vla/09-full-voice-to-action-demo.mdx ```mdx --- sidebar_position: 9 title: "Full Voice→Vision→Action Demo – Say Anything" --- # One Command — Full Conversational Humanoid ```bash # Terminal 1 – Isaac Sim + humanoid ./isaac-sim.sh assets/scenes/kitchen_humanoid.usd # Terminal 2 – Full stack ros2 launch vla voice_to_humanoid.launch.py model:=rdt1b-1.2b-8bit # Terminal 3 – Speak into microphone # → "Please clean up the table and throw away the trash" ``` Watch your humanoid: 1. Understand speech (Whisper-live) 2. Plan with RDT-1B 3. Execute 7-step sequence perfectly **Full source:** `src/vla/voice_to_humanoid/` **You just built a robot that understands natural language better than most humans.** ``` ------- ### Chapter 09 Folder structure (create exactly) ``` docs/ └── 09-sim2real/ ├── 01-intro.mdx ├── 02-the-gap-is-dead.mdx ├── 03-domain-randomization-2025.mdx ├── 04-system-identification.mdx ├── 05-real2sim-pipeline.mdx ├── 06-actuator-modeling.mdx ├── 07-latency-compensation.mdx ├── 08-2025-success-cases.mdx └── 09-full-sim2real-deploy.mdx ``` ### Supporting code & assets ``` src/sim2real/ ├── domain_randomization/ ├── sysid_toolkit/ ├── real2sim_scanner/ └── launch/sim2real_walking.launch.py assets/calibration/ └── unitree_g1_dynamics_2025.yaml ``` ### docs/09-sim2real/_category_.json ```json { "label": "09 – Sim-to-Real Transfer & Domain Randomization", "position": 9, "link": { "type": "generated-index", "description": "The gap is officially dead in 2025 — here's exactly how" } } ``` ### All MDX files — ready to copy-paste #### docs/09-sim2real/01-intro.mdx ```mdx --- sidebar_position: 1 title: "Chapter 9 – Sim-to-Real Transfer in 2025" description: The year simulation finally became more real than reality --- # Chapter 9: Sim-to-Real Transfer & Domain Randomization **Learning Objectives** - Understand why the sim-to-real gap collapsed completely in 2025 - Apply the exact domain randomization recipe used by Figure 02 and Unitree G1 - Perform system identification on any real humanoid in <2 hours - Build a Real→Sim asset pipeline using only an iPhone - Deploy a policy trained 100 % in simulation → real hardware on the first try > By the end, you will take an RL policy trained in Isaac Lab and run it on a real Unitree G1 — with zero failures. ``` #### docs/09-sim2real/02-the-gap-is-dead.mdx ```mdx --- sidebar_position: 2 title: "The Sim-to-Real Gap Is Dead – 2025 Evidence" --- # Real Measured Gaps (Dec 2025) | Task | 2023 Gap (Sim → Real) | 2025 Gap | Status | |-----------------------|------------------------|----------|----------------| | Walking speed | −0.8 m/s | +0.05 m/s| Sim faster | | Push recovery | 80 N → 20 N | 350 N → 340 N| Near perfect | | Grasp success | 92 % → 41 % | 97 % → 96 %| Statistically identical | | Battery insert task | 3/10 success | 10/10 | Solved | **Conclusion: The simulation is now more realistic than the real robot's sensors.** ``` #### docs/09-sim2real/03-domain-randomization-2025.mdx ```mdx --- sidebar_position: 3 title: "Domain Randomization – The 2025 Recipe (Copy-Paste)" --- # Exact Parameters Used by RAISE and RDT-1B ```python # randomization_2025.py DR_RANGES = { "mass": (0.70, 1.40), # × nominal "com_offset": (-0.03, 0.03), # meters "friction": (0.4, 1.6), "restitution": (0.0, 0.6), "joint_damping": (0.5, 3.0), "joint_stiffness": (0.8, 1.3), "latency_ms": (20, 120), "sensor_noise": True, "lighting": "random_hdri", "texture_variation": True } ``` **Just turn this on → 97 % of policies transfer zero-shot.** ``` #### docs/09-sim2real/04-system-identification.mdx ```mdx --- sidebar_position: 4 title: "System Identification – 2-Hour Real Robot Calibration" --- # One-Command SysID ```bash ros2 launch sysid_toolkit excite_motors.launch.py duration:=120 ros2 run sysid_toolkit fit_dynamics.py --data exc_20251208.bag ``` Output: `unitree_g1_calibrated_2025.yaml` with: - Real joint inertia ±0.3 % - Backlash per joint - True friction curves - PD gains that match reality **Used by every top team in 2025.** ``` #### docs/09-sim2real/05-real2sim-pipeline.mdx ```mdx --- sidebar_position: 5 title: "Real→Sim Asset Pipeline – iPhone Only" --- # 30-Second Object Capture ```bash # iPhone → USD in 30 seconds python real2sim_scanner.py --iphone-lidar --object battery ``` Result: Photorealistic, physically accurate USD with correct mass/inertia. Used in RDT-1B training → zero appearance gap. ``` #### docs/09-sim2real/06-actuator-modeling.mdx ```mdx --- sidebar_position: 6 title: "Actuator Modeling – The Last 3 %" --- # 2025 Actuator Model (in Isaac Lab) ```python # Adds real torque ripple + backlash def apply_actuator_net(dof_pos, dof_vel, effort_command): torque = effort_command * torque_constant torque += backlash_hysteresis(dof_pos) torque += velocity_dependent_friction(dof_vel) return torque ``` **Without this → 12 % failure rate. With this → 0.4 %.** ``` #### docs/09-sim2real/07-latency-compensation.mdx ```mdx --- sidebar_position: 7 title: "Latency Compensation – 80 ms Is the New Zero" --- # State Estimator + Smith Predictor ```cpp predicted_state = current_state + velocity * (latency_sec) ``` **All 2025 controllers run with 80–120 ms artificial latency in sim → perfect transfer.** ``` #### docs/09-sim2real/08-2025-success-cases.mdx ```mdx --- sidebar_position: 8 title: "2025 Success Stories – Zero-Shot Real Robot Deploys" --- # Documented Zero-Shot Transfers (Public Videos) | Team | Policy | Training | Real Robot | Success Rate | Date | |--------------------|-------------------|--------------|---------------|--------------|-----------| | RAISE Authors | Walking + push | 10 min sim | Unitree G1 | 100 % | Jan 2025 | | RDT-1B Team | Full kitchen task | 100 % sim | Figure 02 | 10/10 | Mar 2025 | | You (after this chapter) | Your policy | 100 % sim | Your robot | 99+ % | Today | **The era of "it works in sim" excuses is over.** ``` #### docs/09-sim2real/09-full-sim2real-deploy.mdx ```mdx --- sidebar_position: 9 title: "Full Sim→Real Deploy – One Command" --- # From Training to Real Robot in 3 Commands ```bash # 1. Train (already done) # 2. Export policy python export_policy.py --checkpoint rdt1b_final.pt --onnx # 3. Deploy to real Unitree G1 ros2 launch sim2real deploy_to_real.launch.py \ policy:=rdt1b_final.onnx \ robot_ip:=192.168.1.100 \ enable_dr:=false # no need anymore! ``` Watch your real humanoid execute a 7-step task trained 100 % in simulation — first try. **Full source:** `src/sim2real/deploy_to_real/` **You have officially closed the sim-to-real gap. Forever.** ``` ------- ### Chapter 10 Folder structure (create exactly) ``` docs/ └── 10-safety-ethics/ ├── 01-intro.mdx ├── 02-iso-ts-standards.mdx ├── 03-emergency-stop.mdx ├── 04-speed-separation.mdx ├── 05-force-limiting.mdx ├── 06-privacy-and-data.mdx ├── 07-bias-fairness.mdx ├── 08-regulatory-2025.mdx ├── 09-responsible-deployment.mdx └── 10-checklist.mdx ``` ### docs/10-safety-ethics/_category_.json ```json { "label": "10 – Safety, Ethics & Human-Robot Interaction", "position": 10, "link": { "type": "generated-index", "description": "Because super-humanoids must never become super-problems" } } ``` ### All MDX files — ready to copy-paste #### docs/10-safety-ethics/01-intro.mdx ```mdx --- sidebar_position: 1 title: "Chapter 10 – Safety, Ethics & Human-Robot Interaction" description: The non-negotiable chapter before you deploy a 1.8 m, 80 kg humanoid into the real world --- # Chapter 10: Safety, Ethics & Human-Robot Interaction **Learning Objectives** - Implement ISO/TS 15066–compliant safety in ROS 2 - Design emergency-stop and speed & separation monitoring that actually work - Understand force-limiting requirements for human–humanoid contact - Audit your training data for demographic bias - Navigate the EU AI Act, US Executive Order, and China's 2025 robotics regulations - Produce a complete safety case and ethical deployment checklist > A humanoid that can lift 20 kg and walk 2.5 m/s is legally and morally a heavy machine — treat it as such. ``` #### docs/10-safety-ethics/02-iso-ts-standards.mdx ```mdx --- sidebar_position: 2 title: "ISO/TS 15066 & ISO 10218 – What Actually Applies in 2025" --- # Standards That Will Save You From Lawsuits | Standard | Applies to Humanoids? | Key Requirement | 2025 Reality | |-------------------|-----------------------|-------------------------------------|----------------------------------| | ISO 10218-1/2 | Yes | Collaborative robot safety | Mandatory for any shared space | | ISO/TS 15066 | Yes | Power & force limiting (PFL) | Max 150 N transient, 80 N quasi-static | | IEC 61508 SIL-3 | Recommended | Functional safety | Used by Figure, Tesla, Unitree | | IEC 60204-1 | Yes | Emergency stop (Cat 0/1) | Red mushroom button required | **Bottom line:** If your robot weighs >40 kg and moves >0.25 m/s near humans → you are legally a cobot manufacturer. ``` #### docs/10-safety-ethics/03-emergency-stop.mdx ```mdx --- sidebar_position: 3 title: "Emergency Stop – Cat 0 vs Cat 1 in Practice" --- # Production-Grade E-Stop Node (ships with this chapter) ```python # nodes/estop_monitor.py class EStopMonitor(Node): def __init__(self): self.declare_parameter('estop_topic', '/emergency_stop') self.sub = self.create_subscription(Bool, '/emergency_stop', self.cb, 1) self.last_msg = self.get_clock().now() def cb(self, msg): if msg.data: self.get_logger().fatal("E-STOP ACTIVATED → CUTTING POWER") os.system("ros2 service call /servo_power std_srvs/SetBool '{data: false}'") ``` **Hardware requirement:** Dual-channel safety relay wired to motor drivers. ``` #### docs/10-safety-ethics/04-speed-separation.mdx ```mdx --- sidebar_position: 4 title: "Speed & Separation Monitoring – Real Implementation" --- # Live SSM Using PeopleSemSeg + Depth ```python # nodes/ssm_node.py max_speed = 0.0 if closest_human_distance < 0.5: max_speed = 0.0 # stop elif closest_human_distance < 1.5: max_speed = 0.3 # protective speed else: max_speed = 2.5 # full speed self.publish_speed_limit(max_speed) ``` **Tested on Figure 02 fleet — zero collisions in 1.2 million human-hours (2025).** ``` #### docs/10-safety-ethics/05-force-limiting.mdx ```mdx --- sidebar_position: 5 title: "Force Limiting – Torque Monitoring at 1 kHz" --- # Real-Time Torque Safety Layer ```cpp // In joint controller callback (1 kHz) for each joint: measured_torque = current_sensor - gravity_compensation if abs(measured_torque) > TORQUE_LIMIT[joint]: trigger_safety_stop() ``` **2025 limits (ISO/TS 15066 body region map):** - Head/Neck: 65 N / 120 N·s - Torso: 110 N / 190 N·s - Hand/Arm: 140 N / 280 N·s ``` #### docs/10-safety-ethics/06-privacy-and-data.mdx ```mdx --- sidebar_position: 6 title: "Privacy – Always-On Cameras & Microphones" --- # Legal Requirements (EU AI Act 2025) | Requirement | Implementation | |------------------------------|-----------------------------------------------------| | Explicit opt-in recording | LED + physical shutter switch | | On-device processing only | Whisper + VLA run locally, no cloud upload | | Data retention ≤ 24 h | Auto-delete policy in ROS bag recorder | | Right to be forgotten | `ros2 topic pub /delete_user_data std_msgs/Empty` | **Failure to comply = €35M fine per incident.** ``` #### docs/10-safety-ethics/07-bias-fairness.mdx ```mdx --- sidebar_position: 7 title: "Bias & Fairness Audit – Mandatory in 2025" --- # Demographic Bias Checklist | Check | Tool / Metric | Acceptable Threshold | |--------------------------------|-----------------------------------|----------------------| | Skin tone bias in PeopleSeg | Monk Skin Tone scale | ≥90 % on all 10 tones | | Height bias in navigation | Test 1.2 m – 2.1 m humans | No collisions | | Accent bias in Whisper | CommonVoice test set | WER ≤ 12 % all accents | | Object bias (YCB dataset) | Success rate per object class | ≥85 % | **Every top team now publishes a Model Card + Safety Card.** ``` #### docs/10-safety-ethics/08-regulatory-2025.mdx ```mdx --- sidebar_position: 8 title: "Global Regulatory Landscape – December 2025" --- # Where You Can Legally Deploy Today | Region | Classification | Allowed Without Permit | Notes | |--------------|-------------------------|------------------------|-------| | EU | High-risk AI system | Only with CE marking | EU AI Act enforced | | USA | General-purpose robot | Yes (state-level) | Executive Order 14110 | | China | Class III robot | Only in designated zones | Requires NMPA approval | | Singapore | Collaborative robot | Yes | Most permissive | **Recommendation:** Deploy first in Singapore → collect safety data → expand. ``` #### docs/10-safety-ethics/09-responsible-deployment.mdx ```mdx --- sidebar_position: 9 title: "Responsible Deployment Framework" --- # The 7-Step Deployment Checklist (used by Figure, Tesla, Unitree) 1. Safety case document (ISO/TS 15066 compliant) 2. Third-party risk assessment 3. Emergency response plan 4. Operator training certification 5. Data privacy policy + consent forms 6. Bias audit report 7. Incident reporting hotline **Missing any one → do not power on.** ``` #### docs/10-safety-ethics/10-checklist.mdx ```mdx --- sidebar_position: 10 title: "Final Pre-Deployment Checklist (Print & Sign)" --- # Pre-Deployment Safety & Ethics Checklist - [ ] Dual-channel E-Stop tested weekly - [ ] Speed & separation monitoring active - [ ] Force/torque limits enforced in firmware - [ ] Privacy LED + shutter functional - [ ] Bias audit passed on all demographics - [ ] CE / UL safety certification (or equivalent) - [ ] Incident response team on call 24/7 **Only when every box is checked may you power on a humanoid in a human environment.** **You now know more about humanoid safety than 99.9 % of robotics teams worldwide.** ``` ------- ### Chapter 11 **The final chapter. The one that makes everything real.** Folder structure (create exactly) ``` docs/ └── 11-capstone/ ├── 01-overview.mdx ├── 02-scene-and-task.mdx ├── 03-full-pipeline.mdx ├── 04-voice-to-action.mdx ├── 05-planning-and-control.mdx ├── 06-manipulation-sequence.mdx ├── 07-safety-wrapper.mdx ├── 08-one-click-demo.mdx └── 09-extensions.mdx ``` ### Full capstone repository (already included) ``` capstone/ ├── isaac_sim_scenes/ │ └── kitchen_conversational_humanoid.usd ├── ros2_ws/ │ └── src/capstone_bringup/ │ ├── launch/capstone_full.launch.py │ ├── config/params.yaml │ └── nodes/conversational_humanoid_node.py ├── models/ │ └── rdt1b-1.2b-vla-8bit.onnx └── scripts/ └── run_capstone.sh ``` ### docs/11-capstone/_category_.json ```json { "label": "11 – Capstone: Autonomous Conversational Humanoid", "position": 11, "link": { "type": "generated-index", "description": "Say anything → watch a real/simulated humanoid do it perfectly" } } ``` ### All MDX files — ready to copy-paste #### docs/11-capstone/01-overview.mdx ```mdx --- sidebar_position: 1 title: "Capstone: Autonomous Conversational Humanoid" description: The final proof — every chapter combined into one living, talking, thinking robot --- # Capstone Project **Autonomous Conversational Humanoid** **December 2025 — Fully Open, Fully Reproducible** **Task:** Your humanoid wakes up in a kitchen. You say (natural voice): > "Good morning. Please tidy the table, throw away the trash, and bring me a fresh water bottle from the fridge." It understands, plans, walks, manipulates, speaks back, and completes the 9-step task safely. **Everything you learned in Chapters 00–10 is now live in one pipeline.** You will run this today. ``` #### docs/11-capstone/02-scene-and-task.mdx ```mdx --- sidebar_position: 2 title: "The Kitchen Scene & 9-Step Task" --- # The Official 2025 Capstone Scene - 3 × 4 m kitchen (IKEA-style) - Table with 7 random objects (bottle, apple, trash, tools) - Trash bin, sink, fridge (opens) - Human sitting at table (you) - Unitree G1 / Figure-like humanoid (36 DoF, 16 DoF hands) **The 9 steps it must perform autonomously:** 1. Wake up & localize 2. Understand voice command 3. Generate high-level plan 4. Navigate to table 5. Grasp and sort objects 6. Throw trash → bin 7. Open fridge → grasp bottle 8. Bring bottle to human 9. Verbally confirm completion **Success = 9/9 steps, no human intervention, safe at all times.** ``` #### docs/11-capstone/03-full-pipeline.mdx ```mdx --- sidebar_position: 3 title: "Full Pipeline Architecture (2025)" --- ```mermaid graph TD A[USB Mic] --> B(Whisper-live) B --> C(RDT-1B VLA) C --> D(High-Level Planner) D --> E(Nav2 + MoveIt 2) E --> F(RAISE Walking Controller) F --> G(RDT-1B Manipulation) G --> H(Safety Supervisor) H --> I[Real or Simulated Humanoid] I --> J[Synthesized Voice Response] ``` **All components from previous chapters. Zero new magic.** ``` #### docs/11-capstone/04-voice-to-action.mdx ```mdx --- sidebar_position: 4 title: "Voice → Intent (Whisper + RDT-1B)" --- # Real-Time Voice Pipeline ```python # whisper_live + streaming while True: audio_chunk = mic.read() text = whisper_stream.transcribe(audio_chunk) if text.endswith(".") or text.endswith("?"): send_to_vla(text) ``` **RDT-1B receives:** ```text [SYSTEM] You are a helpful home assistant humanoid. [USER] Please tidy the table and bring me a water bottle. ``` → Outputs 2-second action chunks at 28 Hz ``` #### docs/11-capstone/05-planning-and-control.mdx ```mdx --- sidebar_position: 5 title: "High-Level Planning & Execution" --- # LLM Planner (Llama-3.1-70B or Grok) ```python plan = llm_planner.generate( user_command="bring me water", scene_objects=["bottle_blue", "fridge", "table"], robot_capabilities=["walk", "grasp", "open_fridge"] ) # → ["navigate_to(table)", "grasp(bottle)", "navigate_to(fridge)", ...] ``` **Execution manager loops until plan complete or failure.** ``` #### docs/11-capstone/06-manipulation-sequence.mdx ```mdx --- sidebar_position: 6 title: "Manipulation: Grasp → Reorient → Place" --- # All manipulation via RDT-1B ```python for obj in objects_to_move: if obj.class == "trash": goal = "trash_bin" elif obj.class == "bottle": goal = "human_hand" vla_manip.execute(f"pick up the {obj.name} and place it in {goal}") ``` **Success rate (real robot, Dec 2025): 97.3 % across 1000 trials** ``` #### docs/11-capstone/07-safety-wrapper.mdx ```mdx --- sidebar_position: 7 title: "Safety Supervisor – Always Active" --- # Triple-layer safety (never disabled) 1. Speed & Separation Monitoring (Chapter 10) 2. Force/torque limits (firmware) 3. Emergency word trigger ("STOP!" → instant Cat 0) ```python if "stop" in voice_input.lower(): trigger_emergency_stop() ``` **Zero safety incidents in 10,000+ public demos (2025).** ``` #### docs/11-capstone/08-one-click-demo.mdx ```mdx --- sidebar_position: 8 title: "One-Click Demo – Run the Future Now" --- # THREE TERMINALS. THAT'S IT. ```bash # Terminal 1 – Isaac Sim (or real robot) ./isaac-sim.sh capstone/isaac_sim_scenes/kitchen_conversational_humanoid.usd # Terminal 2 – Full ROS 2 stack ros2 launch capstone_bringup capstone_full.launch.py use_sim:=true # Terminal 3 – Speak naturally python speak_to_robot.py # → "Good morning. Please clean the table and bring me a cold water." ``` **Watch 4–7 minutes of pure magic.** **Or deploy to real Unitree G1 / Figure-class robot:** ```bash ros2 launch capstone_bringup capstone_full.launch.py use_sim:=false robot_ip:=192.168.1.42 ``` **You have officially built a real autonomous conversational humanoid — in 2025 — with open tools.** ``` #### docs/11-capstone/09-extensions.mdx ```mdx --- sidebar_position: 9 title: "Next Steps – Make It Yours" --- # Capstone Extensions (Choose Your Adventure) - Add multi-turn dialogue - Teach new objects via 30-second iPhone scan - Deploy two humanoids cooperatively - Run on Jetson Orin edge cluster (no workstation) - Submit to ICRA 2026 "Open Humanoid Challenge" **You are now part of the 0.01 % of humans who have built a truly general-purpose humanoid.** **Congratulations. You did it.** ``` ### Final Words (add to the very end of 09-extensions.mdx) ```mdx --- This textbook began with a vision in early 2025. Eleven chapters later — using only open tools, open models, and open hardware — you have built what the entire robotics community said was "10–20 years away." You proved them wrong. Now go deploy it. The age of Physical AI has begun. And you are holding the manual. — The Physical AI & Humanoid Robotics Textbook Team December 2025 --- ``` **THE TEXTBOOK IS NOW 100% COMPLETE.**"

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Researcher Implements VLA Models (Priority: P1)

As a robotics researcher working on humanoid control in 2025, I want to access comprehensive content on Vision-Language-Action models (VLA) that covers OpenVLA, Octo, and RDT-1B implementations so that I can build a voice-controlled humanoid that responds to natural language commands with >90% success on long-horizon tasks.

**Why this priority**: This covers the core intelligence layer that makes humanoids truly conversational and adaptable to novel situations.

**Independent Test**: The user can successfully run RDT-1B locally and achieve 97% success rate on language-conditioned manipulation tasks.

**Acceptance Scenarios**:

1. **Given** I am implementing OpenVLA for my humanoid, **When** I follow Chapter 8 instructions, **Then** I achieve 94% success on real manipulation tasks with zero fine-tuning
2. **Given** I am using RDT-1B for language understanding, **When** I issue complex commands like "Please tidy the table and bring me a red cup", **Then** the humanoid successfully understands and executes the multi-step task
3. **Given** I am building a voice-to-action pipeline, **When** I integrate Whisper + RDT-1B + ROS2 bridge, **Then** I can command the humanoid with natural speech in real-time

---

### User Story 2 - Engineer Deploys Safe Humanoid (Priority: P2)

As a robotics engineer tasked with deploying humanoids in real environments, I want to access detailed coverage of sim-to-real transfer, safety protocols, and ethical considerations so that I can deploy policies trained 100% in simulation to real hardware with zero safety incidents.

**Why this priority**: Ensuring safe and responsible deployment is critical before any intelligent humanoid operates in human environments.

**Independent Test**: The user can take an RL policy trained in Isaac Lab and run it successfully on a real Unitree G1 with zero failures.

**Acceptance Scenarios**:

1. **Given** I am applying domain randomization techniques, **When** I follow the 2025 recipe, **Then** 97% of policies transfer zero-shot from sim to real without failures
2. **Given** I am implementing safety protocols, **When** I apply ISO/TS 15066 standards, **Then** I achieve compliance and prevent accidents with humans in shared spaces
3. **Given** I am performing system identification, **When** I use the 2-hour calibration process, **Then** I achieve real joint inertia measurements within ±0.3% of actual values

---

### User Story 3 - Practitioner Builds Full System (Priority: P3)

As a robotics practitioner aiming to build a complete autonomous conversational humanoid, I want to access the full capstone project that integrates all concepts from previous chapters so that I can create a humanoid that understands natural voice commands and completes complex tasks autonomously.

**Why this priority**: This represents the ultimate goal of integrating all concepts into a working, real-world system.

**Independent Test**: The user can run the full capstone demo and have the humanoid complete the 9-step task of tidying a table, disposing of trash, and bringing a water bottle from the fridge.

**Acceptance Scenarios**:

1. **Given** I am running the full pipeline, **When** I speak "Good morning. Please tidy the table, throw away the trash, and bring me a fresh water bottle from the fridge", **Then** the humanoid successfully completes all 9 steps safely
2. **Given** I am using the integrated components, **When** I connect Whisper + RDT-1B + RAISE + Navigation, **Then** I achieve seamless execution of multi-modal tasks
3. **Given** I am following the deployment framework, **When** I complete all 7 steps in the checklist, **Then** I can safely power on the humanoid in a human environment

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: All content MUST be in MDX format for Docusaurus compatibility
- **FR-002**: All code examples MUST execute in devcontainer without modification
- **FR-003**: All citations MUST follow IEEE style with minimum 50% peer-reviewed sources
- **FR-004**: Code examples MUST be in Python/ROS 2 as specified
- **FR-005**: Diagrams MUST be created using Mermaid as specified
- **FR-006**: Chapter 8 VLA content MUST include complete coverage of OpenVLA, Octo, RDT-1B, prompt engineering, action chunking, and ROS2 bridges
- **FR-007**: Chapter 9 sim-to-real content MUST include domain randomization recipes, system identification, actuator modeling, and latency compensation techniques
- **FR-008**: Chapter 10 safety content MUST cover ISO/TS 15066 standards, emergency stops, speed/separator monitoring, force limiting, privacy/data protection, and regulatory compliance
- **FR-009**: Chapter 11 capstone project MUST integrate all previous concepts into a working autonomous conversational humanoid system
- **FR-010**: Supporting code assets MUST include all specified directories and files for vla, sim2real, and capstone implementations
- **FR-011**: RDT-1B integration MUST work at 28 Hz inference with 100 Hz control via action chunking
- **FR-012**: Domain randomization parameters MUST match the exact 2025 recipe for 97% zero-shot transfer success
- **FR-013**: Safety protocols MUST comply with ISO/TS 15066 and applicable regional regulations (EU AI Act, US EO, etc.)

### Key Entities

- **Chapter**: Organized content unit within the textbook covering specific topics (08-11 in Module 4)
- **Vision-Language-Action (VLA) Model**: AI model that processes vision, language, and generates actions (e.g., OpenVLA, Octo, RDT-1B)
- **Sim-to-Real Transfer**: Process of transferring policies trained in simulation to real hardware with minimal adaptation
- **Conversational Humanoid**: Humanoid robot that can understand and respond to natural language commands
- **Code Example**: Executable code snippet in Python/ROS 2 that demonstrates concepts
- **Safety Protocol**: Procedures and systems to ensure safe operation of humanoids in human environments
- **Capstone Project**: Comprehensive integration of all concepts into a complete working system

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Reader can run RDT-1B locally and achieve 97% success on long-horizon, language-conditioned tasks
- **SC-002**: Reader can implement domain randomization achieving 97% zero-shot sim-to-real transfer
- **SC-003**: Reader can build voice→vision→action pipeline responding to natural language commands
- **SC-004**: All code executes in devcontainer without modification
- **SC-005**: Reader can take RL policy trained in Isaac Lab to real Unitree G1 with zero failures
- **SC-006**: All chapters are available in proper MDX format with appropriate metadata
- **SC-007**: Supporting code and assets are correctly placed in the repository
- **SC-008**: All content follows IEEE citation style with minimum 50% peer-reviewed sources
- **SC-009**: Capstone project successfully completes 9-step autonomous task with voice commands
- **SC-010**: Safety protocols meet ISO/TS 15066 compliance standards
- **SC-011**: System achieves 28 Hz VLA inference with 100 Hz control via action chunking
- **SC-012**: Policy trained 100% in simulation runs successfully on real hardware on first try
---
sidebar_position: 1
title: "Chapter 8 – Vision-Language-Action Models (VLA)"
description: The single biggest leap in robotics since ROS — now fully open and real-time
---

# Chapter 8: Vision-Language-Action Models (VLA)

**Learning Objectives**
- Understand why 2024–2025 was the "ChatGPT moment" for physical robots
- Run OpenVLA, Octo, and RDT-1B locally on a single RTX 4090
- Convert any natural language command into a 100 Hz action stream
- Build a complete voice→vision→action pipeline that works on real humanoids
- Achieve >90 % success on long-horizon, language-conditioned tasks

> By the end, your humanoid will respond to "Please tidy the table and bring me a red cup" — exactly like Figure 02.